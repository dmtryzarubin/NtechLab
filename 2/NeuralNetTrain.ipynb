{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1600427938363",
   "display_name": "Python 3.7.7 64-bit ('tensorflow': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.callbacks import (EarlyStopping, ModelCheckpoint,\n",
    "                                        ReduceLROnPlateau)\n",
    "from tensorflow.keras.layers import (\n",
    "    Activation, BatchNormalization, Conv2D, Dense, Dropout, Flatten, Input,\n",
    "    LeakyReLU, MaxPooling2D, SeparableConv2D)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Укажем путь к дериктории\n",
    "PATH = 'internship_data'\n",
    "MALE = os.path.join(PATH,'male')\n",
    "FEMALE = os.path.join(PATH,'female')\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "source": [
    "##   Посмотрим, сколько изображений и каких размеров в датасете, для выбора размера входного слоя в модели."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 50001/50001 [00:55<00:00, 902.25it/s]\n100%|██████████| 50001/50001 [00:46<00:00, 1083.38it/s]\n"
    }
   ],
   "source": [
    "img_sizes = dict()\n",
    "\n",
    "i = 0\n",
    "\n",
    "for picture in tqdm(os.listdir(FEMALE)):\n",
    "    if '.jpg' in picture:\n",
    "        im = Image.open(os.path.join(FEMALE,picture))\n",
    "        \n",
    "        if im.size in img_sizes.keys():\n",
    "            img_sizes[im.size] +=1\n",
    "        else:\n",
    "            img_sizes[im.size] = 1\n",
    "for picture in tqdm(os.listdir(MALE)):\n",
    "    if '.jpg' in picture:\n",
    "        im = Image.open(os.path.join(MALE,picture))\n",
    "        \n",
    "        if im.size in img_sizes.keys():\n",
    "            img_sizes[im.size] +=1\n",
    "        else:\n",
    "            img_sizes[im.size] = 1\n",
    "img_sizes_sorted = sorted(img_sizes.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "source": [
    "## Посмотрим на 10 самых популярных:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nРазмер изображения:  (70, 94) \nКоличество изображений с таким размером: 56\n\nРазмер изображения:  (98, 127) \nКоличество изображений с таким размером: 53\n\nРазмер изображения:  (77, 100) \nКоличество изображений с таким размером: 52\n\nРазмер изображения:  (101, 133) \nКоличество изображений с таким размером: 52\n\nРазмер изображения:  (84, 112) \nКоличество изображений с таким размером: 52\n\nРазмер изображения:  (75, 97) \nКоличество изображений с таким размером: 51\n\nРазмер изображения:  (95, 126) \nКоличество изображений с таким размером: 50\n\nРазмер изображения:  (86, 115) \nКоличество изображений с таким размером: 50\n\nРазмер изображения:  (93, 125) \nКоличество изображений с таким размером: 49\n\nРазмер изображения:  (87, 115) \nКоличество изображений с таким размером: 49\n"
    }
   ],
   "source": [
    "schet = 0 \n",
    "for i in img_sizes_sorted:\n",
    "    if schet != 10:\n",
    "        print('\\nРазмер изображения: ', i[0], \n",
    "              '\\nКоличество изображений с таким размером:',\n",
    "              i[1])\n",
    "        schet+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "source": [
    "### Больше всего изображений с размером 70х94, но тем не менее их достаточно мало, чтобы принять решение конкретно размеров входных данных, следовательно нужно будет  сжимать или растягивать изображения до оптимального размера."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Укажем размер для входных изображений, batch_size, и название модели."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "IMG_HEIGHT = 96\n",
    "IMG_WIDTH = 96\n",
    "model_name = 'Gender_2'"
   ]
  },
  {
   "source": [
    "## Создадим генератор входных изображений, который будет подгружать фотографии в модель, во время обучения. Также разделим датасет в пропорции 30/70 на валидацию и обучение."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_generator = ImageDataGenerator(rescale=1./255, \n",
    "                        validation_split = 0.30, rotation_range=40,\n",
    "                        width_shift_range=0.2, height_shift_range=0.2, \n",
    "                        shear_range=0.2, zoom_range=0.2, \n",
    "                        horizontal_flip=True, fill_mode='nearest')\n",
    "                        # каждый раз изображения будут искажаться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Found 70002 images belonging to 2 classes.\n\nFound 30000 images belonging to 2 classes.\n"
    }
   ],
   "source": [
    "# Генератор для тренировочной выборки\n",
    "train_data_gen = train_image_generator.flow_from_directory(batch_size = batch_size,\n",
    "                                                           directory=PATH,\n",
    "                                                           shuffle=True,\n",
    "                                                           target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                           class_mode='binary',\n",
    "                                                           subset = 'training')\n",
    "print()\n",
    "# Генератор для валидационной выборки \n",
    "val_data_gen = train_image_generator.flow_from_directory(batch_size=batch_size,\n",
    "                                                         directory = PATH,\n",
    "                                                         target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                         class_mode='binary',\n",
    "                                                         subset = 'validation')"
   ]
  },
  {
   "source": [
    "## Создадим модель свёрточной нейронной сети"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_tensor = Input(shape = (IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "#============================================================================================================\n",
    "layer1 = layers.Conv2D(filters = 16, kernel_size = (3, 3), \n",
    "                        padding = 'same', activation = 'relu')(input_tensor)\n",
    "layer1 = layers.BatchNormalization()(layer1)\n",
    "layer1 = layers.MaxPooling2D()(layer1)\n",
    "\n",
    "#============================================================================================================\n",
    "\n",
    "layer2 = layers.Conv2D(filters = 32, kernel_size = (3, 3), \n",
    "                        padding = 'same', activation = 'relu')(layer1)\n",
    "layer2 = layers.BatchNormalization()(layer2)\n",
    "layer3 = layers.MaxPooling2D()(layer2)\n",
    "\n",
    "#============================================================================================================\n",
    "\n",
    "layer4 = layers.Conv2D(filters = 64, kernel_size = (3, 3), \n",
    "                        padding = 'same', activation = 'relu')(layer3)\n",
    "layer4 = layers.BatchNormalization()(layer4)\n",
    "layer4 = layers.MaxPooling2D()(layer4)\n",
    "\n",
    "#============================================================================================================\n",
    "\n",
    "layer5 = layers.Conv2D(filters = 128, kernel_size = (3, 3), \n",
    "                        padding = 'same', activation = 'relu')(layer4)\n",
    "layer5 = layers.BatchNormalization()(layer5)\n",
    "layer5 = layers.MaxPooling2D()(layer5)\n",
    "\n",
    "#============================================================================================================\n",
    "\n",
    "layer6 = layers.Flatten()(layer5)\n",
    "\n",
    "#============================================================================================================\n",
    "\n",
    "layer7 = layers.Dense(256, activation = 'relu')(layer6)\n",
    "\n",
    "#============================================================================================================\n",
    "\n",
    "layer8 = layers.Dense(1, activation = 'sigmoid')(layer7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_tensor, layer8)"
   ]
  },
  {
   "source": [
    "## Пропишем обратные вызовы для сохранения промежуточных моделей и прекращения обучения при обучении."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [ tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience = 2,verbose = 1),\n",
    "                                # Обучение будет остановлено если функция потерь на валидации не будет\n",
    "                                # падать в течение 3- х эпох.\n",
    "                   \n",
    "                   tf.keras.callbacks.ModelCheckpoint( \n",
    "                                filepath =  model_name +'.h5', \n",
    "                                monitor='val_accuracy',save_best_only=True, mode = 'max', verbose = 1),\n",
    "                                # Будет сохраняться модель с наилучшей точностью на валидации  \n",
    "                   \n",
    "                   tf.keras.callbacks.ReduceLROnPlateau( \n",
    "                                monitor='val_loss', factor = 0.1, \n",
    "                                patience = 1, verbose = 1),\n",
    "                                # Скорость обучения будет уменьшаться в 10 раз, если от эпохи к эпохе\n",
    "                                # функция потерь на валидации не будет увличиваться в течение 2-х эпох\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# оптимизатор - Adam \n",
    "# функция потерь - кросс энтропия\n",
    "# метрика - доля правильных ответов\n",
    "model.compile(optimizer = 'adam', loss ='binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "source": [
    "## Посмотрим на модель."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 96, 96, 3)]       0         \n_________________________________________________________________\nconv2d (Conv2D)              (None, 96, 96, 16)        448       \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 96, 96, 16)        64        \n_________________________________________________________________\nmax_pooling2d (MaxPooling2D) (None, 48, 48, 16)        0         \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 48, 48, 32)        4640      \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 48, 48, 32)        128       \n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n_________________________________________________________________\nconv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n_________________________________________________________________\nbatch_normalization_2 (Batch (None, 24, 24, 64)        256       \n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n_________________________________________________________________\nconv2d_3 (Conv2D)            (None, 12, 12, 128)       73856     \n_________________________________________________________________\nbatch_normalization_3 (Batch (None, 12, 12, 128)       512       \n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 4608)              0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               1179904   \n_________________________________________________________________\ndense_1 (Dense)              (None, 1)                 257       \n=================================================================\nTotal params: 1,278,561\nTrainable params: 1,278,081\nNon-trainable params: 480\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "source": [
    "## Обучение модели."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Epoch 1/15\n545/546 [============================>.] - ETA: 0s - loss: 0.3399 - accuracy: 0.8519\nEpoch 00001: val_accuracy improved from -inf to 0.87804, saving model to Gender_2.h5\n546/546 [==============================] - 283s 518ms/step - loss: 0.3397 - accuracy: 0.8519 - val_loss: 0.2881 - val_accuracy: 0.8780\nEpoch 2/15\n545/546 [============================>.] - ETA: 0s - loss: 0.1855 - accuracy: 0.9236\nEpoch 00002: val_accuracy improved from 0.87804 to 0.91179, saving model to Gender_2.h5\n546/546 [==============================] - 298s 546ms/step - loss: 0.1854 - accuracy: 0.9236 - val_loss: 0.2214 - val_accuracy: 0.9118\nEpoch 3/15\n545/546 [============================>.] - ETA: 0s - loss: 0.1539 - accuracy: 0.9375\nEpoch 00003: val_accuracy improved from 0.91179 to 0.91787, saving model to Gender_2.h5\n546/546 [==============================] - 286s 524ms/step - loss: 0.1538 - accuracy: 0.9375 - val_loss: 0.2018 - val_accuracy: 0.9179\nEpoch 4/15\n545/546 [============================>.] - ETA: 0s - loss: 0.1395 - accuracy: 0.9442\nEpoch 00004: val_accuracy improved from 0.91787 to 0.92728, saving model to Gender_2.h5\n546/546 [==============================] - 291s 533ms/step - loss: 0.1395 - accuracy: 0.9442 - val_loss: 0.1791 - val_accuracy: 0.9273\nEpoch 5/15\n545/546 [============================>.] - ETA: 0s - loss: 0.1279 - accuracy: 0.9494\nEpoch 00005: val_accuracy improved from 0.92728 to 0.94224, saving model to Gender_2.h5\n546/546 [==============================] - 291s 533ms/step - loss: 0.1278 - accuracy: 0.9494 - val_loss: 0.1423 - val_accuracy: 0.9422\nEpoch 6/15\n545/546 [============================>.] - ETA: 0s - loss: 0.1205 - accuracy: 0.9533\nEpoch 00006: val_accuracy did not improve from 0.94224\n\nEpoch 00006: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n546/546 [==============================] - 289s 530ms/step - loss: 0.1206 - accuracy: 0.9533 - val_loss: 0.2003 - val_accuracy: 0.9260\nEpoch 7/15\n545/546 [============================>.] - ETA: 0s - loss: 0.1015 - accuracy: 0.9607\nEpoch 00007: val_accuracy improved from 0.94224 to 0.95653, saving model to Gender_2.h5\n546/546 [==============================] - 292s 535ms/step - loss: 0.1016 - accuracy: 0.9607 - val_loss: 0.1123 - val_accuracy: 0.9565\nEpoch 8/15\n545/546 [============================>.] - ETA: 0s - loss: 0.0956 - accuracy: 0.9632\nEpoch 00008: val_accuracy improved from 0.95653 to 0.96030, saving model to Gender_2.h5\n546/546 [==============================] - 288s 527ms/step - loss: 0.0956 - accuracy: 0.9632 - val_loss: 0.1043 - val_accuracy: 0.9603\nEpoch 9/15\n545/546 [============================>.] - ETA: 0s - loss: 0.0914 - accuracy: 0.9647\nEpoch 00009: val_accuracy improved from 0.96030 to 0.96104, saving model to Gender_2.h5\n546/546 [==============================] - 281s 515ms/step - loss: 0.0913 - accuracy: 0.9648 - val_loss: 0.1027 - val_accuracy: 0.9610\nEpoch 10/15\n545/546 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9659\nEpoch 00010: val_accuracy improved from 0.96104 to 0.96127, saving model to Gender_2.h5\n546/546 [==============================] - 282s 517ms/step - loss: 0.0897 - accuracy: 0.9659 - val_loss: 0.1024 - val_accuracy: 0.9613\nEpoch 11/15\n545/546 [============================>.] - ETA: 0s - loss: 0.0897 - accuracy: 0.9661\nEpoch 00011: val_accuracy improved from 0.96127 to 0.96227, saving model to Gender_2.h5\n546/546 [==============================] - 276s 505ms/step - loss: 0.0896 - accuracy: 0.9662 - val_loss: 0.0979 - val_accuracy: 0.9623\nEpoch 12/15\n545/546 [============================>.] - ETA: 0s - loss: 0.0862 - accuracy: 0.9676\nEpoch 00012: val_accuracy did not improve from 0.96227\n\nEpoch 00012: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n546/546 [==============================] - 276s 505ms/step - loss: 0.0862 - accuracy: 0.9676 - val_loss: 0.0999 - val_accuracy: 0.9619\nEpoch 13/15\n545/546 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.9671\nEpoch 00013: val_accuracy improved from 0.96227 to 0.96404, saving model to Gender_2.h5\n546/546 [==============================] - 277s 506ms/step - loss: 0.0866 - accuracy: 0.9671 - val_loss: 0.0964 - val_accuracy: 0.9640\nEpoch 14/15\n545/546 [============================>.] - ETA: 0s - loss: 0.0841 - accuracy: 0.9679\nEpoch 00014: val_accuracy did not improve from 0.96404\n546/546 [==============================] - 276s 505ms/step - loss: 0.0841 - accuracy: 0.9679 - val_loss: 0.0958 - val_accuracy: 0.9635\nEpoch 15/15\n545/546 [============================>.] - ETA: 0s - loss: 0.0834 - accuracy: 0.9684\nEpoch 00015: val_accuracy improved from 0.96404 to 0.96471, saving model to Gender_2.h5\n546/546 [==============================] - 274s 502ms/step - loss: 0.0833 - accuracy: 0.9684 - val_loss: 0.0945 - val_accuracy: 0.9647\n"
    }
   ],
   "source": [
    "hist = model.fit_generator(\n",
    "    train_data_gen,\n",
    "    steps_per_epoch = train_data_gen.samples // batch_size,\n",
    "    epochs = 15,\n",
    "    validation_data = val_data_gen,\n",
    "    validation_steps= val_data_gen.samples // batch_size,\n",
    "    callbacks = callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "{'female': 0, 'male': 1}\n"
    }
   ],
   "source": [
    "# Метки для каждого класса\n",
    "label_map = (train_data_gen.class_indices)\n",
    "print(label_map)"
   ]
  }
 ]
}